{"cells":[{"cell_type":"markdown","source":["# Introduction to Spark programming"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7cb3ddc-3abe-456d-bb1b-2e1a0c18dc2b"}}},{"cell_type":"markdown","source":["# Word Count\n\nThe \"Hello World!\" of distributed programming is the wordcount. Basically, you want to count easily number of different words contained in an unstructured text. You will write some code to perform this task on the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) retrieved from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page).\n\n[Spark's Python API reference](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) could provide some help\n\n### ** Part 1: Creating a base RDD and pair RDDs **\n\n#### In this part of the lab, we will explore creating a base RDD with `parallelize` and using pair RDDs to count words.\n\n#### We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2be5ef54-b710-444e-87e6-6b8e72d28a31"}}},{"cell_type":"code","source":["#Please, run this first\nimport hashlib\nimport sys\n\ndef hash(x):\n  return hashlib.sha1(str(x).encode('utf-8')).hexdigest()\n\nassert sys.version_info.major == 3"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6beead7-f55a-4126-a12b-55393c0eb4bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["words_list = ['we', 'few', 'we', 'happy', 'few', \"we\", \"band\", \"of\", \"brothers\"]\nwords_RDD = sc.parallelize(words_list, 4)\n\n# Print the type of words_RDD\nprint(type(words_RDD))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfd05341-a514-4e7f-afd0-6cd7cb39d8c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<class 'pyspark.rdd.RDD'>\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["<class 'pyspark.rdd.RDD'>\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["We want to capitalize each word contained in a RDD. For such transformation, we use a `map`, as we want to transform a RDD of **n** elements into another RDD of **n** using a function that gets and returns one single element.\n\nPlease implement `capitalize`function in the cell below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5b72398-65d4-4f58-9a7d-e9050aba31bc"}}},{"cell_type":"code","source":["def capitalize(word):\n    \"\"\"Capitalize lowercase `words`.\n\n    Args:\n        word (str): A lowercase string.\n\n    Returns:\n        str: A string which first letter is uppercase.\n    \"\"\"\n    return word[0].upper() + word[1:]\n\nprint(capitalize('we'))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"424413a6-c049-423e-8df1-f034ffcaf249"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"We\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["We\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Apply `capitalize` to the base RDD, using a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `capitalize()` function to each element. Then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to retrieve the values of the transformed RDD, and print them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92fac128-b30b-4145-a48f-f83b05bee880"}}},{"cell_type":"code","source":["local_result = words_RDD.map(capitalize).collect()\n\nprint(local_result)\n\nassert hash(local_result) == 'bd73c54004cc9655159aceb703bc14fe93369fb1',\\\n       'Capitalize'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be8245ca-5036-4927-a32d-d2b5c24a4a4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Do the same using a lambda function"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"047b882f-0417-495a-a983-129f388a4e21"}}},{"cell_type":"code","source":["local_result = words_RDD.map(lambda x: x.capitalize()).collect()\n\nprint(local_result)\n\nassert hash(local_result) == 'bd73c54004cc9655159aceb703bc14fe93369fb1',\\\n       'Capitalize with lambda'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c315dcb-3e32-4129-a901-420dda3c28e4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['We', 'Few', 'We', 'Happy', 'Few', 'We', 'Band', 'Of', 'Brothers']\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Now use `map()` and a `lambda` function to return the number of characters in each word, and `collect` this result directly into a variable."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d08606a2-55e6-4cc2-aff3-30551d44eb2c"}}},{"cell_type":"code","source":["plural_lengths = (words_RDD.map(lambda x: len(x)).collect())\n\nprint(plural_lengths)\n\nassert hash(plural_lengths) == '0772853c8e180c1bed8cfe9bde35aae79b277381',\\\n       'incorrect values for plural_lengths'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5c287ce-746d-4d87-afad-e646b97d7f6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[2, 3, 2, 5, 3, 2, 4, 2, 8]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[2, 3, 2, 5, 3, 2, 4, 2, 8]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["To program a wordcount, we will need `pair RDD` objects. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.\n\nCreate the pair RDD using the `map()` transformation with a `lambda()` on `words_RDD`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ac2fb77-12b5-4018-9f83-d44b41b1f177"}}},{"cell_type":"code","source":["words_pair_RDD = words_RDD.map(lambda x: (x, 1))\n\nprint(words_pair_RDD.collect())\n\nassert hash(words_pair_RDD.collect()) == 'fb67a530034e01395386569ef29bf5565b503ec6', \\\n       'incorrect value for words_pair_RDD'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb3eb422-f9dc-4836-9dc3-8d5e49e5e289"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('we', 1), ('few', 1), ('we', 1), ('happy', 1), ('few', 1), ('we', 1), ('band', 1), ('of', 1), ('brothers', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('we', 1), ('few', 1), ('we', 1), ('happy', 1), ('few', 1), ('we', 1), ('band', 1), ('of', 1), ('brothers', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient or scalable than others.\n\nA naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, it is not scalable as the result of `collect()` would have to fit in the driver's memory. When you should use `collect()` with care, always asking yourself what is the size of data you want to retrieve.\n\nIn order to program a scalable wordcount, you will need to use parallel operations.\n\n#### `groupByKey()` approach\n\nAn approach you might first consider is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. This transformation groups all the elements of the RDD with the same key into a single list, stored in one of the partitions. \n \nUse `groupByKey()` on `words_pair_RDD`\n to generate a pair RDD of type `('word', list)`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c83f8a1e-617c-4bdc-92ed-9fe5710a53a3"}}},{"cell_type":"code","source":["words_grouped = words_pair_RDD.groupByKey()\n\nfor key, value in words_grouped.collect():\n    print('{0}: {1}'.format(key, list(value)))\n    \nassert hash(sorted(words_grouped.mapValues(lambda x: list(x)).collect())) == 'fdaad77fd81ef2df23d98ff7fd438fa700ca1fcf',\\\n       'incorrect value for words_grouped'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"57c63f94-b67a-4138-9b50-813472ba11de"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"of: [1]\nfew: [1, 1]\nbrothers: [1]\nwe: [1, 1, 1]\nband: [1]\nhappy: [1]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["of: [1]\nfew: [1, 1]\nbrothers: [1]\nwe: [1, 1, 1]\nband: [1]\nhappy: [1]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Using the `groupByKey()` transformation results in an `pairRDD` containing words as keys, and Python iterators as values. Python iterators are a class of objects on which we can iterate, i.e.\n\n    a = some_iterator()\n    for elem in a:\n        # do stuff with elem\n\nPython lists and dictionnaries are iterators for example.\n\nNow sum the iterator using a `map()` transformation. The result should be a pair RDD consisting of (word, count) pairs.\n\nHint: there exists a `sum` function\nHint 2: you want to perform an operation only on the values of the pairRDD. Take a look at [mapValues()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0afefc4-b8d0-4f7d-ba01-5540ce6b0970"}}},{"cell_type":"code","source":["word_grouped_counts = words_grouped.mapValues(lambda x: sum(x))\n\nprint(word_grouped_counts.collect())\n\nassert hash(sorted(word_grouped_counts.collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\\\n       'incorrect value for word_grouped_counts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8f6d6d1-15a4-44c2-b264-5a2a60a8dd51"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["There are two problems with using `groupByKey()`:\n  + The operation requires a lot of data movement to move all the values into the appropriate partitions (remember the cost of network communications!).\n  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory of a worker.\n\nA better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n\nCompute the word count using `reduceByKey`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a84de21f-fa8c-473f-9960-ebba7ff4a211"}}},{"cell_type":"code","source":["from operator import add\nword_counts = words_pair_RDD.reduceByKey(add)\n\nprint(word_counts.collect())\n\nassert hash(sorted(word_counts.collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\\\n       'incorrect value for word_counts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da5dd9a-1414-4546-8848-c0c8208bcd11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You should be able to perform the word count by composing functions, resulting in a smaller code. Use the `map()` on word RDD to create a pair RDD, apply the `reduceByKey()` transformation, and `collect` in one statement."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6edcb5d1-22fb-4f68-a677-90d3f77bab80"}}},{"cell_type":"code","source":["word_counts_collected = (words_RDD\n                         .map(lambda x: (x, 1))\n                         .reduceByKey(add)\n                         .collect()\n                        )\n\nprint(word_counts_collected)\n\nassert hash(sorted(word_counts_collected)) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a', \\\n       'incorrect value for word_counts_collected'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20312b70-910b-4e46-974d-dc8f7af1f2f1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Compute the number of unique words using one of the RDD you have already created."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f9c97c8-ac35-42cd-b81e-7af085cb342f"}}},{"cell_type":"code","source":["unique_words = words_RDD.distinct().count()\n#unique_words = len(word_counts_collected)\n\nprint(unique_words)\n\nassert unique_words == 6, 'incorrect count of unique_words'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08b1ec16-1520-44a1-923f-2cc649c4d445"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"6\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["6\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words to find the mean number of words per unique word in `word_counts`.  First `map()` RDD `word_counts`, which consists of (key, value) pairs, to an RDD of values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed6e3d04-4d23-4d47-9b41-a919951d5c26"}}},{"cell_type":"code","source":["total_count = (word_counts\n              .map(lambda x: x[1])\n              .reduce(add))\n\naverage = total_count / float(unique_words)\n\nprint(total_count)\nprint(round(average, 2))\n\nassert round(average, 2) == 1.5, 'incorrect value of average'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fd16b70-26d4-4dec-936d-c9154a0573a8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"9\n1.5\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["9\n1.5\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["## Part 2: Apply word count to a file\n\nIn this section we will finish developing our word count application.  We'll have to build the `word_count` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data.\n\nFirst, define a function for word counting. You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in an RDD that is a list of words like `words_RDD` and return a pair RDD that has all of the words and their associated counts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11bb9fdf-5c57-43d5-a1f3-e03e326d7898"}}},{"cell_type":"code","source":["def word_count(word_list_RDD):\n    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n\n    Args:\n        wordListRDD (RDD of str): An RDD consisting of words.\n\n    Returns:\n        RDD of (str, int): An RDD consisting of (word, count) tuples.\n    \"\"\"\n    pair_RDD = word_list_RDD.map(lambda x: (x, 1)).reduceByKey(add)\n    return pair_RDD\n\nprint(word_count(words_RDD).collect())\n\nassert hash(sorted(word_count(words_RDD).collect())) == 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a', \\\n       'incorrect definition for word_count function'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e696f93c-5c0f-4d51-b862-b893e036a0a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["[('of', 1), ('few', 2), ('brothers', 1), ('we', 3), ('band', 1), ('happy', 1)]\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Real world data is more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n  + All punctuation should be removed.\n  + Any leading or trailing spaces on a line should be removed.\n \nDefine the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. Reading `help(re.sub)` might be useful.\n\nIf you have never used regex (regular expressions) before, you can refer to [Regular-expressions.info](http://www.regular-expressions.info/python.html)\n\nIn order to test your regular expressions, you can use [Regex Tester](http://www.regexpal.com)\n\nRegex can be a bit obscure at beginning, don't hesitate to search in [StackOverflow](http://stackoverflow.com) or to ask me for some help."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9fd50d95-3c43-4a8f-883b-a719d8d82015"}}},{"cell_type":"code","source":["import re\nimport string\n\n# Hint: string.punctuation contains all the punctuation symbols\n\ndef remove_punctuation(text):\n    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n\n    Note:\n        Only spaces, letters, and numbers should be retained.  Other characters should should be\n        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n        punctuation is removed.\n\n    Args:\n        text (str): A string.\n\n    Returns:\n        str: The cleaned up string.\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^\\w\\s]\", \"\", text) # removes punctuation\n    text = text.replace(\"_\", \"\")\n    text = text.strip()\n    \n    return text\n\nprint(remove_punctuation('Hello World!'))\nprint(remove_punctuation(' No under_score!'))\n\nassert remove_punctuation(\"  Remove punctuation: there ARE trailing spaces. \") == 'remove punctuation there are trailing spaces', \\\n       'incorrect definition for remove_punctuation function'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58e61136-f020-48d2-b0e5-f80d8560a70f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"hello world\nno underscore\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["hello world\nno underscore\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `remove_punctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, instead of `collect()` so that we only print 15 lines.\n\nTake a look at [zipWithIndex()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) and [take()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) to understand the print statement.\n\nTo understand how we load the data, look at [databricks documentation](https://docs.databricks.com/user-guide/importing-data.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"60d31b23-ee79-48bb-a7a5-73cdb6d82c7a"}}},{"cell_type":"code","source":["%sh wget https://waterponey.github.io/BigDataClass/files/shakespeare.txt -O shakespeare.txt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"110b9a08-71b2-44c7-9b56-9669e66cab44"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"--2022-01-10 17:09:33--  https://waterponey.github.io/BigDataClass/files/shakespeare.txt\nResolving waterponey.github.io (waterponey.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\nConnecting to waterponey.github.io (waterponey.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5589889 (5.3M) [text/plain]\nSaving to: ‘shakespeare.txt’\n\n     0K .......... .......... .......... .......... ..........  0% 6.12M 1s\n    50K .......... .......... .......... .......... ..........  1% 4.37M 1s\n   100K .......... .......... .......... .......... ..........  2% 6.23M 1s\n   150K .......... .......... .......... .......... ..........  3% 17.5M 1s\n   200K .......... .......... .......... .......... ..........  4%  120M 1s\n   250K .......... .......... .......... .......... ..........  5% 75.2M 1s\n   300K .......... .......... .......... .......... ..........  6% 10.8M 1s\n   350K .......... .......... .......... .......... ..........  7% 53.5M 0s\n   400K .......... .......... .......... .......... ..........  8% 6.18M 0s\n   450K .......... .......... .......... .......... ..........  9%  147M 0s\n   500K .......... .......... .......... .......... .......... 10% 99.8M 0s\n   550K .......... .......... .......... .......... .......... 10% 13.8M 0s\n   600K .......... .......... .......... .......... .......... 11% 61.4M 0s\n   650K .......... .......... .......... .......... .......... 12%  102M 0s\n   700K .......... .......... .......... .......... .......... 13%  113M 0s\n   750K .......... .......... .......... .......... .......... 14%  103M 0s\n   800K .......... .......... .......... .......... .......... 15% 69.3M 0s\n   850K .......... .......... .......... .......... .......... 16% 9.73M 0s\n   900K .......... .......... .......... .......... .......... 17%  135M 0s\n   950K .......... .......... .......... .......... .......... 18% 63.8M 0s\n  1000K .......... .......... .......... .......... .......... 19% 95.9M 0s\n  1050K .......... .......... .......... .......... .......... 20% 68.1M 0s\n  1100K .......... .......... .......... .......... .......... 21% 42.2M 0s\n  1150K .......... .......... .......... .......... .......... 21%  143M 0s\n  1200K .......... .......... .......... .......... .......... 22% 44.1M 0s\n  1250K .......... .......... .......... .......... .......... 23% 9.28M 0s\n  1300K .......... .......... .......... .......... .......... 24% 68.2M 0s\n  1350K .......... .......... .......... .......... .......... 25%  145M 0s\n  1400K .......... .......... .......... .......... .......... 26% 89.4M 0s\n  1450K .......... .......... .......... .......... .......... 27% 16.6M 0s\n  1500K .......... .......... .......... .......... .......... 28% 16.5M 0s\n  1550K .......... .......... .......... .......... .......... 29%  155M 0s\n  1600K .......... .......... .......... .......... .......... 30% 80.1M 0s\n  1650K .......... .......... .......... .......... .......... 31% 76.1M 0s\n  1700K .......... .......... .......... .......... .......... 32%  152M 0s\n  1750K .......... .......... .......... .......... .......... 32%  117M 0s\n  1800K .......... .......... .......... .......... .......... 33% 68.1M 0s\n  1850K .......... .......... .......... .......... .......... 34%  107M 0s\n  1900K .......... .......... .......... .......... .......... 35%  118M 0s\n  1950K .......... .......... .......... .......... .......... 36% 71.3M 0s\n  2000K .......... .......... .......... .......... .......... 37%  131M 0s\n  2050K .......... .......... .......... .......... .......... 38%  118M 0s\n  2100K .......... .......... .......... .......... .......... 39% 71.1M 0s\n  2150K .......... .......... .......... .......... .......... 40%  149M 0s\n  2200K .......... .......... .......... .......... .......... 41%  123M 0s\n  2250K .......... .......... .......... .......... .......... 42% 27.0M 0s\n  2300K .......... .......... .......... .......... .......... 43% 84.3M 0s\n  2350K .......... .......... .......... .......... .......... 43%  116M 0s\n  2400K .......... .......... .......... .......... .......... 44%  126M 0s\n  2450K .......... .......... .......... .......... .......... 45% 76.0M 0s\n  2500K .......... .......... .......... .......... .......... 46%  147M 0s\n  2550K .......... .......... .......... .......... .......... 47%  116M 0s\n  2600K .......... .......... .......... .......... .......... 48% 67.0M 0s\n  2650K .......... .......... .......... .......... .......... 49%  138M 0s\n  2700K .......... .......... .......... .......... .......... 50%  106M 0s\n  2750K .......... .......... .......... .......... .......... 51% 72.2M 0s\n  2800K .......... .......... .......... .......... .......... 52%  119M 0s\n  2850K .......... .......... .......... .......... .......... 53%  156M 0s\n  2900K .......... .......... .......... .......... .......... 54%  119M 0s\n  2950K .......... .......... .......... .......... .......... 54% 17.4M 0s\n  3000K .......... .......... .......... .......... .......... 55%  112M 0s\n  3050K .......... .......... .......... .......... .......... 56% 78.6M 0s\n  3100K .......... .......... .......... .......... .......... 57%  154M 0s\n  3150K .......... .......... .......... .......... .......... 58%  159M 0s\n  3200K .......... .......... .......... .......... .......... 59%  144M 0s\n  3250K .......... .......... .......... .......... .......... 60%  155M 0s\n  3300K .......... .......... .......... .......... .......... 61%  159M 0s\n  3350K .......... .......... .......... .......... .......... 62%  148M 0s\n  3400K .......... .......... .......... .......... .......... 63%  132M 0s\n  3450K .......... .......... .......... .......... .......... 64% 94.5M 0s\n  3500K .......... .......... .......... .......... .......... 65% 87.3M 0s\n  3550K .......... .......... .......... .......... .......... 65%  122M 0s\n  3600K .......... .......... .......... .......... .......... 66%  115M 0s\n  3650K .......... .......... .......... .......... .......... 67%  114M 0s\n  3700K .......... .......... .......... .......... .......... 68%  161M 0s\n  3750K .......... .......... .......... .......... .......... 69%  108M 0s\n  3800K .......... .......... .......... .......... .......... 70%  120M 0s\n  3850K .......... .......... .......... .......... .......... 71%  165M 0s\n  3900K .......... .......... .......... .......... .......... 72%  139M 0s\n  3950K .......... .......... .......... .......... .......... 73%  152M 0s\n  4000K .......... .......... .......... .......... .......... 74% 81.0M 0s\n  4050K .......... .......... .......... .......... .......... 75%  110M 0s\n  4100K .......... .......... .......... .......... .......... 76%  151M 0s\n  4150K .......... .......... .......... .......... .......... 76% 97.7M 0s\n  4200K .......... .......... .......... .......... .......... 77% 81.4M 0s\n  4250K .......... .......... .......... .......... .......... 78%  122M 0s\n  4300K .......... .......... .......... .......... .......... 79%  120M 0s\n  4350K .......... .......... .......... .......... .......... 80%  147M 0s\n  4400K .......... .......... .......... .......... .......... 81% 88.9M 0s\n  4450K .......... .......... .......... .......... .......... 82%  156M 0s\n  4500K .......... .......... .......... .......... .......... 83%  155M 0s\n  4550K .......... .......... .......... .......... .......... 84%  148M 0s\n  4600K .......... .......... .......... .......... .......... 85% 85.1M 0s\n  4650K .......... .......... .......... .......... .......... 86%  113M 0s\n  4700K .......... .......... .......... .......... .......... 87%  150M 0s\n  4750K .......... .......... .......... .......... .......... 87%  107M 0s\n  4800K .......... .......... .......... .......... .......... 88% 66.0M 0s\n  4850K .......... .......... .......... .......... .......... 89%  149M 0s\n  4900K .......... .......... .......... .......... .......... 90% 92.4M 0s\n  4950K .......... .......... .......... .......... .......... 91% 73.5M 0s\n  5000K .......... .......... .......... .......... .......... 92% 90.6M 0s\n  5050K .......... .......... .......... .......... .......... 93% 74.5M 0s\n  5100K .......... .......... .......... .......... .......... 94%  136M 0s\n  5150K .......... .......... .......... .......... .......... 95% 95.7M 0s\n  5200K .......... .......... .......... .......... .......... 96% 70.8M 0s\n  5250K .......... .......... .......... .......... .......... 97%  140M 0s\n  5300K .......... .......... .......... .......... .......... 98% 89.4M 0s\n  5350K .......... .......... .......... .......... .......... 98% 72.6M 0s\n  5400K .......... .......... .......... .......... .......... 99% 91.0M 0s\n  5450K ........                                              100%  168M=0.1s\n\n2022-01-10 17:09:33 (47.0 MB/s) - ‘shakespeare.txt’ saved [5589889/5589889]\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["--2022-01-10 17:09:33--  https://waterponey.github.io/BigDataClass/files/shakespeare.txt\nResolving waterponey.github.io (waterponey.github.io)... 185.199.110.153, 185.199.111.153, 185.199.108.153, ...\nConnecting to waterponey.github.io (waterponey.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5589889 (5.3M) [text/plain]\nSaving to: ‘shakespeare.txt’\n\n     0K .......... .......... .......... .......... ..........  0% 6.12M 1s\n    50K .......... .......... .......... .......... ..........  1% 4.37M 1s\n   100K .......... .......... .......... .......... ..........  2% 6.23M 1s\n   150K .......... .......... .......... .......... ..........  3% 17.5M 1s\n   200K .......... .......... .......... .......... ..........  4%  120M 1s\n   250K .......... .......... .......... .......... ..........  5% 75.2M 1s\n   300K .......... .......... .......... .......... ..........  6% 10.8M 1s\n   350K .......... .......... .......... .......... ..........  7% 53.5M 0s\n   400K .......... .......... .......... .......... ..........  8% 6.18M 0s\n   450K .......... .......... .......... .......... ..........  9%  147M 0s\n   500K .......... .......... .......... .......... .......... 10% 99.8M 0s\n   550K .......... .......... .......... .......... .......... 10% 13.8M 0s\n   600K .......... .......... .......... .......... .......... 11% 61.4M 0s\n   650K .......... .......... .......... .......... .......... 12%  102M 0s\n   700K .......... .......... .......... .......... .......... 13%  113M 0s\n   750K .......... .......... .......... .......... .......... 14%  103M 0s\n   800K .......... .......... .......... .......... .......... 15% 69.3M 0s\n   850K .......... .......... .......... .......... .......... 16% 9.73M 0s\n   900K .......... .......... .......... .......... .......... 17%  135M 0s\n   950K .......... .......... .......... .......... .......... 18% 63.8M 0s\n  1000K .......... .......... .......... .......... .......... 19% 95.9M 0s\n  1050K .......... .......... .......... .......... .......... 20% 68.1M 0s\n  1100K .......... .......... .......... .......... .......... 21% 42.2M 0s\n  1150K .......... .......... .......... .......... .......... 21%  143M 0s\n  1200K .......... .......... .......... .......... .......... 22% 44.1M 0s\n  1250K .......... .......... .......... .......... .......... 23% 9.28M 0s\n  1300K .......... .......... .......... .......... .......... 24% 68.2M 0s\n  1350K .......... .......... .......... .......... .......... 25%  145M 0s\n  1400K .......... .......... .......... .......... .......... 26% 89.4M 0s\n  1450K .......... .......... .......... .......... .......... 27% 16.6M 0s\n  1500K .......... .......... .......... .......... .......... 28% 16.5M 0s\n  1550K .......... .......... .......... .......... .......... 29%  155M 0s\n  1600K .......... .......... .......... .......... .......... 30% 80.1M 0s\n  1650K .......... .......... .......... .......... .......... 31% 76.1M 0s\n  1700K .......... .......... .......... .......... .......... 32%  152M 0s\n  1750K .......... .......... .......... .......... .......... 32%  117M 0s\n  1800K .......... .......... .......... .......... .......... 33% 68.1M 0s\n  1850K .......... .......... .......... .......... .......... 34%  107M 0s\n  1900K .......... .......... .......... .......... .......... 35%  118M 0s\n  1950K .......... .......... .......... .......... .......... 36% 71.3M 0s\n  2000K .......... .......... .......... .......... .......... 37%  131M 0s\n  2050K .......... .......... .......... .......... .......... 38%  118M 0s\n  2100K .......... .......... .......... .......... .......... 39% 71.1M 0s\n  2150K .......... .......... .......... .......... .......... 40%  149M 0s\n  2200K .......... .......... .......... .......... .......... 41%  123M 0s\n  2250K .......... .......... .......... .......... .......... 42% 27.0M 0s\n  2300K .......... .......... .......... .......... .......... 43% 84.3M 0s\n  2350K .......... .......... .......... .......... .......... 43%  116M 0s\n  2400K .......... .......... .......... .......... .......... 44%  126M 0s\n  2450K .......... .......... .......... .......... .......... 45% 76.0M 0s\n  2500K .......... .......... .......... .......... .......... 46%  147M 0s\n  2550K .......... .......... .......... .......... .......... 47%  116M 0s\n  2600K .......... .......... .......... .......... .......... 48% 67.0M 0s\n  2650K .......... .......... .......... .......... .......... 49%  138M 0s\n  2700K .......... .......... .......... .......... .......... 50%  106M 0s\n  2750K .......... .......... .......... .......... .......... 51% 72.2M 0s\n  2800K .......... .......... .......... .......... .......... 52%  119M 0s\n  2850K .......... .......... .......... .......... .......... 53%  156M 0s\n  2900K .......... .......... .......... .......... .......... 54%  119M 0s\n  2950K .......... .......... .......... .......... .......... 54% 17.4M 0s\n  3000K .......... .......... .......... .......... .......... 55%  112M 0s\n  3050K .......... .......... .......... .......... .......... 56% 78.6M 0s\n  3100K .......... .......... .......... .......... .......... 57%  154M 0s\n  3150K .......... .......... .......... .......... .......... 58%  159M 0s\n  3200K .......... .......... .......... .......... .......... 59%  144M 0s\n  3250K .......... .......... .......... .......... .......... 60%  155M 0s\n  3300K .......... .......... .......... .......... .......... 61%  159M 0s\n  3350K .......... .......... .......... .......... .......... 62%  148M 0s\n  3400K .......... .......... .......... .......... .......... 63%  132M 0s\n  3450K .......... .......... .......... .......... .......... 64% 94.5M 0s\n  3500K .......... .......... .......... .......... .......... 65% 87.3M 0s\n  3550K .......... .......... .......... .......... .......... 65%  122M 0s\n  3600K .......... .......... .......... .......... .......... 66%  115M 0s\n  3650K .......... .......... .......... .......... .......... 67%  114M 0s\n  3700K .......... .......... .......... .......... .......... 68%  161M 0s\n  3750K .......... .......... .......... .......... .......... 69%  108M 0s\n  3800K .......... .......... .......... .......... .......... 70%  120M 0s\n  3850K .......... .......... .......... .......... .......... 71%  165M 0s\n  3900K .......... .......... .......... .......... .......... 72%  139M 0s\n  3950K .......... .......... .......... .......... .......... 73%  152M 0s\n  4000K .......... .......... .......... .......... .......... 74% 81.0M 0s\n  4050K .......... .......... .......... .......... .......... 75%  110M 0s\n  4100K .......... .......... .......... .......... .......... 76%  151M 0s\n  4150K .......... .......... .......... .......... .......... 76% 97.7M 0s\n  4200K .......... .......... .......... .......... .......... 77% 81.4M 0s\n  4250K .......... .......... .......... .......... .......... 78%  122M 0s\n  4300K .......... .......... .......... .......... .......... 79%  120M 0s\n  4350K .......... .......... .......... .......... .......... 80%  147M 0s\n  4400K .......... .......... .......... .......... .......... 81% 88.9M 0s\n  4450K .......... .......... .......... .......... .......... 82%  156M 0s\n  4500K .......... .......... .......... .......... .......... 83%  155M 0s\n  4550K .......... .......... .......... .......... .......... 84%  148M 0s\n  4600K .......... .......... .......... .......... .......... 85% 85.1M 0s\n  4650K .......... .......... .......... .......... .......... 86%  113M 0s\n  4700K .......... .......... .......... .......... .......... 87%  150M 0s\n  4750K .......... .......... .......... .......... .......... 87%  107M 0s\n  4800K .......... .......... .......... .......... .......... 88% 66.0M 0s\n  4850K .......... .......... .......... .......... .......... 89%  149M 0s\n  4900K .......... .......... .......... .......... .......... 90% 92.4M 0s\n  4950K .......... .......... .......... .......... .......... 91% 73.5M 0s\n  5000K .......... .......... .......... .......... .......... 92% 90.6M 0s\n  5050K .......... .......... .......... .......... .......... 93% 74.5M 0s\n  5100K .......... .......... .......... .......... .......... 94%  136M 0s\n  5150K .......... .......... .......... .......... .......... 95% 95.7M 0s\n  5200K .......... .......... .......... .......... .......... 96% 70.8M 0s\n  5250K .......... .......... .......... .......... .......... 97%  140M 0s\n  5300K .......... .......... .......... .......... .......... 98% 89.4M 0s\n  5350K .......... .......... .......... .......... .......... 98% 72.6M 0s\n  5400K .......... .......... .......... .......... .......... 99% 91.0M 0s\n  5450K ........                                              100%  168M=0.1s\n\n2022-01-10 17:09:33 (47.0 MB/s) - ‘shakespeare.txt’ saved [5589889/5589889]\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["file_path = 'file:/databricks/driver/shakespeare.txt'\n\nshakespeare_RDD = (sc.textFile(file_path, 8)\n                     .map(remove_punctuation))\n\nprint('\\n'.join(shakespeare_RDD\n                .zipWithIndex()  # to (line, lineNum) pairRDD\n                .map(lambda pair: '{0}: {1}'.format(pair[1], pair[0]))  # to 'lineNum: line'\n                .take(15)))"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a595885e-2e8e-4ecc-a4c2-3faf3f978a37"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"0: the project gutenberg ebook of the complete works of william shakespeare by\n1: william shakespeare\n2: \n3: this ebook is for the use of anyone anywhere at no cost and with\n4: almost no restrictions whatsoever  you may copy it give it away or\n5: reuse it under the terms of the project gutenberg license included\n6: with this ebook or online at wwwgutenbergorg\n7: \n8: this is a copyrighted project gutenberg ebook details below\n9: please follow the copyright guidelines in this file\n10: \n11: title the complete works of william shakespeare\n12: \n13: author william shakespeare\n14: \n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["0: the project gutenberg ebook of the complete works of william shakespeare by\n1: william shakespeare\n2: \n3: this ebook is for the use of anyone anywhere at no cost and with\n4: almost no restrictions whatsoever  you may copy it give it away or\n5: reuse it under the terms of the project gutenberg license included\n6: with this ebook or online at wwwgutenbergorg\n7: \n8: this is a copyrighted project gutenberg ebook details below\n9: please follow the copyright guidelines in this file\n10: \n11: title the complete works of william shakespeare\n12: \n13: author william shakespeare\n14: \n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Before we can use the `word_count()` function, we have to address two issues with the format of the RDD:\n  + #### The first issue is that  that we need to split each line by its spaces.\n  + #### The second issue is we need to filter out empty lines.\n \nApply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation is the way to do this, but think about what the result of the `split()` function will be: there is a better option.\n\nHint: remember the problem we had with `GroupByKey()`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa6b9054-b37d-4812-825f-8dbfcdb29693"}}},{"cell_type":"code","source":["shakespeare_words_RDD = shakespeare_RDD.flatMap(lambda x: x.split())\nshakespeare_word_count_elem = shakespeare_words_RDD.count()\n\nprint(shakespeare_words_RDD.take(5))\nprint(shakespeare_word_count_elem)\n\n#assert shakespeare_word_count_elem == 1010679, \\\n#       'incorrect value for shakespeare_word_count_elem'\n\n#assert hash(shakespeare_words_RDD.top(5)) == '036f886dad6af36a651fd51910180a9a0e267d84', \\\n#       'incorrect value for shakespeare_words_RDD'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b17ca3f-e876-4e63-819a-fc3b2e0cfd16"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"['the', 'project', 'gutenberg', 'ebook', 'of']\n903705\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["['the', 'project', 'gutenberg', 'ebook', 'of']\n903705\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["The next step is to filter out the empty elements.  Remove all entries where the word is `''`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"519f54a3-8ca5-4ea1-b763-a9049fcd87e2"}}},{"cell_type":"code","source":["shakespeare_nonempty_words_RDD = shakespeare_words_RDD.filter(lambda x: x != \"\")\nshakespeare_nonempty_word_elem_count = shakespeare_nonempty_words_RDD.count()\n\nprint(shakespeare_nonempty_word_elem_count)\n\n#assert shakespeare_nonempty_word_elem_count == 960028, \\\n#       'incorrect value for shakespeare_nonempty_word_elem_count'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"451bdfdb-d2cd-402f-afeb-fef5e4edfd2e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"903705\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["903705\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You now have an RDD that contains only words.  Next, apply the `word_count()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action. However, since the elements of the RDD are pairs, you will need a custom sort function that sorts using the value part of the pair.\n\nUse the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5986fcc-7152-4f72-b516-19b03e305993"}}},{"cell_type":"code","source":["top15_words = shakespeare_nonempty_words_RDD.map(lambda x: word_count(x)).takeOrdered(15)\n\nprint('\\n'.join(map(lambda pair: '{0}: {1}'.format(pair[0], pair[1]), top15_words)))\n\nassert hash(top15_words) == '3c4a0974cc09536ba8902683e0ad441d8239a710', \\\n       'incorrect value for top15WordsAndCounts'"],"metadata":{"collapsed":false,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77775941-c160-476e-b0b9-1ae9db2987ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4229865841361056>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtop15_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mshakespeare_nonempty_words_RDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mword_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtakeOrdered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'{0}: {1}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mhash\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'3c4a0974cc09536ba8902683e0ad441d8239a710'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtakeOrdered\u001B[0;34m(self, num, key)\u001B[0m\n\u001B[1;32m   1539\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mheapq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnsmallest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1541\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mheapq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnsmallest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1030\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1032\u001B[0;31m         \u001B[0mvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1033\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1034\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 967\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    968\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 113.0 failed 1 times, most recent failure: Lost task 2.0 in stage 113.0 (TID 375) (ip-10-172-233-50.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'AttributeError: 'str' object has no attribute 'map'', from <command-4229865841361045>, line 10. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1541, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in nsmallest\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in <listcomp>\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/databricks/spark/python/pyspark/util.py\", line 72, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-4229865841361056>\", line 1, in <lambda>\n  File \"<command-4229865841361045>\", line 10, in word_count\nAttributeError: 'str' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor376.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'AttributeError: 'str' object has no attribute 'map'', from <command-4229865841361045>, line 10. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1541, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in nsmallest\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in <listcomp>\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/databricks/spark/python/pyspark/util.py\", line 72, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-4229865841361056>\", line 1, in <lambda>\n  File \"<command-4229865841361045>\", line 10, in word_count\nAttributeError: 'str' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 113.0 failed 1 times, most recent failure: Lost task 2.0 in stage 113.0 (TID 375) (ip-10-172-233-50.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'AttributeError: 'str' object has no attribute 'map'', from <command-4229865841361045>, line 10. Full traceback below:","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-4229865841361056>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtop15_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mshakespeare_nonempty_words_RDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mword_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtakeOrdered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m15\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'\\n'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m'{0}: {1}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpair\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32massert\u001B[0m \u001B[0mhash\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtop15_words\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m'3c4a0974cc09536ba8902683e0ad441d8239a710'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mtakeOrdered\u001B[0;34m(self, num, key)\u001B[0m\n\u001B[1;32m   1539\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mheapq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnsmallest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1540\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1541\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mheapq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnsmallest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmerge\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1542\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1543\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mtake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mreduce\u001B[0;34m(self, f)\u001B[0m\n\u001B[1;32m   1030\u001B[0m             \u001B[0;32myield\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minitial\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1031\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1032\u001B[0;31m         \u001B[0mvals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmapPartitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1033\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1034\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mreduce\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/rdd.py\u001B[0m in \u001B[0;36mcollect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    965\u001B[0m         \u001B[0;31m# Default path used in OSS Spark / for non-credential passthrough clusters:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    966\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mSCCallSiteSync\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontext\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcss\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 967\u001B[0;31m             \u001B[0msock_info\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jvm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPythonRDD\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcollectAndServe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrdd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    968\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_load_from_socket\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msock_info\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jrdd_deserializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    969\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 113.0 failed 1 times, most recent failure: Lost task 2.0 in stage 113.0 (TID 375) (ip-10-172-233-50.us-west-2.compute.internal executor driver): org.apache.spark.api.python.PythonException: 'AttributeError: 'str' object has no attribute 'map'', from <command-4229865841361045>, line 10. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1541, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in nsmallest\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in <listcomp>\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/databricks/spark/python/pyspark/util.py\", line 72, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-4229865841361056>\", line 1, in <lambda>\n  File \"<command-4229865841361045>\", line 10, in word_count\nAttributeError: 'str' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2828)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2775)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2769)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2769)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1305)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1305)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3036)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2965)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1067)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2476)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1036)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:419)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1034)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor376.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: 'AttributeError: 'str' object has no attribute 'map'', from <command-4229865841361045>, line 10. Full traceback below:\nTraceback (most recent call last):\n  File \"/databricks/spark/python/pyspark/worker.py\", line 741, in main\n    process()\n  File \"/databricks/spark/python/pyspark/worker.py\", line 731, in process\n    out_iter = func(split_index, iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 2953, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 419, in func\n    return f(iterator)\n  File \"/databricks/spark/python/pyspark/rdd.py\", line 1541, in <lambda>\n    return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in nsmallest\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/usr/lib/python3.8/heapq.py\", line 488, in <listcomp>\n    result = [(elem, i) for i, elem in zip(range(n), it)]\n  File \"/databricks/spark/python/pyspark/util.py\", line 72, in wrapper\n    return f(*args, **kwargs)\n  File \"<command-4229865841361056>\", line 1, in <lambda>\n  File \"<command-4229865841361045>\", line 10, in word_count\nAttributeError: 'str' object has no attribute 'map'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:641)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:781)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:763)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:594)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1038)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:813)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1605)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:816)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:672)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["You will notice that many of the words are common English words. These are called stopwords. In practice, when we do natural language processing, we filter these stopwords as they do not contain a lot of information."],"metadata":{"collapsed":true,"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e55e92e-eb9d-45a3-847a-9405d2bfde61"}}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.9","nbconvert_exporter":"python","file_extension":".py"},"name":"2-spark-rdd","notebookId":1361410605051095,"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"application/vnd.databricks.v1+notebook":{"notebookName":"3-spark-rdd","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":4229865841361017}},"nbformat":4,"nbformat_minor":0}
